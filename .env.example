# LLM API Configuration
# Your API key for the LLM service
OPENAI_API_KEY=your_api_key_here

# Optional: Custom API base URL (for Ollama, Nebius, etc.)
# Leave empty or comment out for standard OpenAI
# OPENAI_API_BASE=http://localhost:11434/v1
# OPENAI_API_BASE=https://api.studio.nebius.com/v1/

# LLM Model Configuration
# Examples: gpt-4o, gpt-3.5-turbo, llama3, mistral, deepseek-ai/DeepSeek-V3
LLM_MODEL_NAME=gpt-4o

# LLM Parameters
# Temperature: 0.0 for deterministic, 1.0 for creative (recommended: 0.0 for extraction)
LLM_TEMPERATURE=0.0

# Maximum tokens for LLM response
LLM_MAX_TOKENS=4096

# Text Processing Configuration
# Number of words per chunk
CHUNK_SIZE=150

# Number of words to overlap between chunks
CHUNK_OVERLAP=30

# Visualization Configuration (optional)
# Layout algorithm: cose, circle, grid, breadthfirst, concentric
GRAPH_LAYOUT=cose

# Enable/disable animation
ANIMATE_LAYOUT=true